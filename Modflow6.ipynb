{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import MODFLOW 6 models from Geomodelr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Geomodelr webinar. Here we are going to explain how to load and run our models created and exported from Geomodelr. Furthemore, we will show how to get the results and save them in the _vtk_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this goal, we need the last version of flopy (3.2.12). Furthemore, we are using the vtk library, version 8.2.0, json library, version 2.0.9 and numpy, version 1.16.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flopy is installed in D:\\Programas_Instalados\\Anaconda3\\envs\\advanced-math\\lib\\site-packages\\flopy\n",
      "numpy version 1.16.4\n",
      "flopy version 3.2.12\n",
      "json version 2.0.9\n",
      "vtk version 8.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import flopy\n",
    "import vtk\n",
    "import json\n",
    "from vtk.util import numpy_support\n",
    "\n",
    "print('numpy version {}'.format(np.__version__))\n",
    "print('flopy version {}'.format(flopy.__version__))\n",
    "print('json version {}'.format(json.__version__))\n",
    "print(vtk.vtkVersion.GetVTKSourceVersion())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load simulation\n",
    "\n",
    "We define the name of the simulation and its location. On the other hand, we must to set the location of mf6.exe (including the executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'mf6_webinar' # Name of the study\n",
    "\n",
    "sim_location = 'D:/CompartidaVB/Modflow/Webinar_2019/modflow6/{}/'.format(file_name)\n",
    "exe_location = 'C:/WRDAPP/mf6.0.4/mf6.0.4/bin/mf6.exe' # mf6 exucatable location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we load the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading simulation...\n",
      "  loading simulation name file...\n",
      "  loading tdis package...\n",
      "  loading model gwf6...\n",
      "    loading package disu...\n",
      "    loading package riv...\n",
      "    loading package wel...\n",
      "    loading package npf...\n",
      "    loading package ic...\n",
      "    loading package oc...\n",
      "  loading ims package mf6_webinar...\n"
     ]
    }
   ],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=sim_location , exe_name=exe_location)\n",
    "model = sim.get_model(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model packges: ['disu', 'riv_0', 'wel', 'npf', 'ic', 'oc']\n",
      "\n",
      "Simulation packges: dict_keys(['nam', 'tdis', 'ims'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_packages = model.package_names #All packages of the model\n",
    "print('Model packges: {}\\n'.format(model_packages))\n",
    "\n",
    "sim_packages = sim.package_key_dict.keys()\n",
    "print('Simulation packges: {}\\n'.format(sim_packages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISU Package\n",
    "It is the package that contains all the data about mesh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells info:\n",
      "\n",
      "[(     0, 1067330.93, 1073391.66, 7,     7,     6,     5,     4, 3, 2, 1, None, None, None, None)\n",
      " (     1, 1067330.93, 1073391.66, 7,     7,     6,     5,     4, 3, 2, 1, None, None, None, None)\n",
      " (     2, 1067330.93, 1073391.66, 7,     7,     6,     5,     4, 3, 2, 1, None, None, None, None)\n",
      " ...\n",
      " (358476, 1066760.63, 1063718.83, 4, 47886, 42517, 42518, 45406, None, None, None, None, None, None, None)\n",
      " (358477, 1066760.63, 1063718.83, 4, 47886, 42517, 42518, 45406, None, None, None, None, None, None, None)\n",
      " (358478, 1066760.63, 1063718.83, 4, 47886, 42517, 42518, 45406, None, None, None, None, None, None, None)]\n"
     ]
    }
   ],
   "source": [
    "disu = model.get_package('disu')\n",
    "\n",
    "print('Cells info:\\n\\n{}'.format(disu.cell2d.array)) # Cells array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices info:\n",
      "\n",
      "[(    0, 1067350.66, 1073402.91) (    1, 1067347.78, 1073405.99)\n",
      " (    2, 1067333.06, 1073406.41) ... (47883, 1066699.42, 1063707.81)\n",
      " (47884, 1066723.9 , 1063707.81) (47885, 1066748.39, 1063707.81)]\n"
     ]
    }
   ],
   "source": [
    "print('Vertices info:\\n\\n{}'.format(disu.vertices.array)) # vertices array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conection info:\t[    -1      2     14 ... 341218 358463 358478]\n"
     ]
    }
   ],
   "source": [
    "print('Conection info:\\t{}'.format(disu.ja.array)) # vertices array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riv package\n",
    "This package contains all corresponding nodes to rivers. These data are organized as (node, stage, conductance, bottom, name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{internal}\n",
      "([((1396,), 0., 0., 0., 'river_0') ((1410,), 0., 0., 0., 'river_0')\n",
      " ((1425,), 0., 0., 0., 'river_0') ... ((10620,), 0., 0., 0., 'river_15')\n",
      " ((347908,), 0., 0., 0., 'river_15') ((347923,), 0., 0., 0., 'river_15')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "riv = model.get_package('riv')\n",
    "print(riv.stress_period_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Geomodelr exports rivers data (stage, conductance and bottom) equal to 0.0 and the names are enumerates from \"river_0\" to \"river_(n-1)\", where n is the number of rivers. To edit the rivers data , we have to load rivers_name.json file. This file contains the original name the of the rivers (names in the shapefile) and its correponding enumerated name. We do this to avoid runtime errors because non-ASCII names are not allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rivers names:\n",
      "{'river_12': 'CD_EL_MOLINO', 'river_13': 'Q._CAIBO', 'river_10': 'CD_MALPASO', 'river_11': 'RIO_ICABUCO', 'river_14': 'Q._BLANCA', 'river_15': 'Q._LOS_CONFINES', 'river_8': 'Q_EL_BARRIAL', 'river_9': 'CD_EL_ROSAL', 'river_0': 'CD_LAS_LARA', 'river_1': 'Q._VOLADOR', 'river_2': 'Q._RUBIO', 'river_3': 'RIO_BOSQUE', 'river_4': 'Q._LA_LAJA', 'river_5': 'Q_TASVITA', 'river_6': 'Q._BOSQUE', 'river_7': 'Q._COLORADA'}\n"
     ]
    }
   ],
   "source": [
    "json_file = sim_location + 'rivers_name.json'\n",
    "with open(json_file) as json_file:  \n",
    "    river_names = json.load(json_file)\n",
    "print(\"Rivers names:\\n{}\".format(river_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is usefull if the stage, conductance and bottom is different for each river or if the user wants to chaged the river names to the original ones (if they are ASCII strings), but in this case, we will define the rivers parameters as follow:\n",
    "- stage = top - 0.5 (the stage of the river is located 50cm under the topography)\n",
    "- conductance = 30.\n",
    "- bottom = top - 2.5 (the bottom of the river is located 2.5m under the topography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{internal}\n",
      "([((1396,), 2615.56543162, 30., 2613.56543162, 'CD_LAS_LARA')\n",
      " ((1410,), 2603.10831491, 30., 2601.10831491, 'CD_LAS_LARA')\n",
      " ((1425,), 2598.00491308, 30., 2596.00491308, 'CD_LAS_LARA') ...\n",
      " ((10620,), 1918.98375071, 30., 1916.98375071, 'Q._LOS_CONFINES')\n",
      " ((347908,), 1916.61170247, 30., 1914.61170247, 'Q._LOS_CONFINES')\n",
      " ((347923,), 1920.30177343, 30., 1918.30177343, 'Q._LOS_CONFINES')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top = disu.top.array\n",
    "for data in riv.stress_period_data.array[0]:\n",
    "    node = data[0][0]\n",
    "    data[1] = top[node] - 0.5\n",
    "    data[2] = 30.\n",
    "    data[3] = top[node] - 2.5\n",
    "    enum_name = data[4]\n",
    "    data[4] = river_names[enum_name]\n",
    "\n",
    "print(riv.stress_period_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wel package\n",
    "This package contains all data about wells: wells nodes,  and ther names\n",
    "This package contains all corresponding nodes to wells. These data are organized as (node, volumetric well rate, name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{internal}\n",
      "([((24155,), 0., 'well_0') ((24156,), 0., 'well_0')\n",
      " ((24157,), 0., 'well_0') ((24158,), 0., 'well_0')\n",
      " ((24159,), 0., 'well_0') ((24160,), 0., 'well_0')\n",
      " ((24161,), 0., 'well_0') ((24162,), 0., 'well_0')\n",
      " ((24163,), 0., 'well_0') ((24164,), 0., 'well_0')\n",
      " ((24165,), 0., 'well_0') ((24166,), 0., 'well_0')\n",
      " ((24167,), 0., 'well_0') ((24168,), 0., 'well_0')\n",
      " ((24169,), 0., 'well_0') ((24170,), 0., 'well_0')\n",
      " ((24171,), 0., 'well_0') ((24172,), 0., 'well_0')\n",
      " ((24173,), 0., 'well_0') ((24197,), 0., 'well_1')\n",
      " ((24198,), 0., 'well_1') ((24199,), 0., 'well_1')\n",
      " ((24200,), 0., 'well_1') ((24201,), 0., 'well_1')\n",
      " ((24202,), 0., 'well_1') ((24203,), 0., 'well_1')\n",
      " ((24204,), 0., 'well_1') ((24205,), 0., 'well_1')\n",
      " ((24206,), 0., 'well_1') ((24207,), 0., 'well_1')\n",
      " ((24208,), 0., 'well_1') ((24209,), 0., 'well_1')\n",
      " ((24210,), 0., 'well_1') ((24211,), 0., 'well_1')\n",
      " ((24212,), 0., 'well_1') ((24213,), 0., 'well_1')\n",
      " ((24214,), 0., 'well_1') ((24215,), 0., 'well_1')\n",
      " ((24238,), 0., 'well_2') ((24239,), 0., 'well_2')\n",
      " ((24240,), 0., 'well_2') ((24241,), 0., 'well_2')\n",
      " ((24242,), 0., 'well_2') ((24243,), 0., 'well_2')\n",
      " ((24244,), 0., 'well_2') ((24245,), 0., 'well_2')\n",
      " ((24246,), 0., 'well_2') ((24247,), 0., 'well_2')\n",
      " ((24248,), 0., 'well_2') ((24249,), 0., 'well_2')\n",
      " ((24266,), 0., 'well_3') ((24267,), 0., 'well_3')\n",
      " ((24268,), 0., 'well_3') ((24269,), 0., 'well_3')\n",
      " ((24270,), 0., 'well_3') ((24271,), 0., 'well_3')\n",
      " ((24272,), 0., 'well_3') ((24273,), 0., 'well_3')\n",
      " ((24274,), 0., 'well_3') ((24302,), 0., 'well_4')\n",
      " ((24303,), 0., 'well_4') ((24333,), 0., 'well_5')\n",
      " ((24334,), 0., 'well_5') ((24335,), 0., 'well_5')\n",
      " ((24336,), 0., 'well_5') ((24337,), 0., 'well_5')\n",
      " ((24338,), 0., 'well_5') ((24339,), 0., 'well_5')\n",
      " ((24340,), 0., 'well_5') ((24341,), 0., 'well_5')\n",
      " ((24342,), 0., 'well_5') ((24343,), 0., 'well_5')\n",
      " ((24344,), 0., 'well_5') ((24345,), 0., 'well_5')\n",
      " ((24381,), 0., 'well_6') ((24382,), 0., 'well_6')\n",
      " ((24383,), 0., 'well_6') ((24384,), 0., 'well_6')\n",
      " ((24385,), 0., 'well_6') ((24386,), 0., 'well_6')\n",
      " ((24387,), 0., 'well_6') ((24388,), 0., 'well_6')\n",
      " ((24389,), 0., 'well_6') ((24390,), 0., 'well_6')\n",
      " ((24391,), 0., 'well_6') ((24392,), 0., 'well_6')\n",
      " ((24393,), 0., 'well_6') ((24394,), 0., 'well_6')\n",
      " ((24395,), 0., 'well_6') ((24396,), 0., 'well_6')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wel = model.get_package('wel')\n",
    "print(wel.stress_period_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like rivers data, Geomodelr exports the volumetric well rate for each well equal to 0.0, and the names of the rivers are enumerate from \"well_0\" to \"well_(m-1)\", where _m_ is the number of wells in the model. Then, we are going to create a dictyonary using the original name of the wells and their corresponding volumetric rates. Furthemore, we load the wells-name.json file to get the enumerated well name and its corresponding name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wells names:\n",
      "{'well_3': 'chinavita_1', 'well_2': 'umbita_2', 'well_1': 'umbita_1', 'well_0': 'tibana_3', 'well_6': 'chinavita_2', 'well_5': 'tibana_2', 'well_4': 'tibana_1'}\n",
      "\n",
      "Volumetric well rates:\n",
      "{'chinavita_1': -12.0, 'umbita_1': -13.0, 'tibana_1': -14.0, 'chinavita_2': -15.0, 'umbita_2': -16.0, 'tibana_2': -17.0, 'tibana_3': -18.0}\n"
     ]
    }
   ],
   "source": [
    "json_file = sim_location + 'wells_name.json'\n",
    "with open(json_file) as json_file:  \n",
    "    well_names = json.load(json_file)\n",
    "    \n",
    "print(\"Wells names:\\n{}\\n\".format(well_names))\n",
    "\n",
    "wells_rate = {u'chinavita_1': -12., u'umbita_1': -13., u'tibana_1': -14., u'chinavita_2': -15.,\n",
    "\t\t  u'umbita_2': -16., u'tibana_2': -17., u'tibana_3': -18.}\n",
    "\n",
    "print('Volumetric well rates:\\n{}'.format(wells_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{internal}\n",
      "([((24155,), -18., 'tibana_3') ((24156,), -18., 'tibana_3')\n",
      " ((24157,), -18., 'tibana_3') ((24158,), -18., 'tibana_3')\n",
      " ((24159,), -18., 'tibana_3') ((24160,), -18., 'tibana_3')\n",
      " ((24161,), -18., 'tibana_3') ((24162,), -18., 'tibana_3')\n",
      " ((24163,), -18., 'tibana_3') ((24164,), -18., 'tibana_3')\n",
      " ((24165,), -18., 'tibana_3') ((24166,), -18., 'tibana_3')\n",
      " ((24167,), -18., 'tibana_3') ((24168,), -18., 'tibana_3')\n",
      " ((24169,), -18., 'tibana_3') ((24170,), -18., 'tibana_3')\n",
      " ((24171,), -18., 'tibana_3') ((24172,), -18., 'tibana_3')\n",
      " ((24173,), -18., 'tibana_3') ((24197,), -13., 'umbita_1')\n",
      " ((24198,), -13., 'umbita_1') ((24199,), -13., 'umbita_1')\n",
      " ((24200,), -13., 'umbita_1') ((24201,), -13., 'umbita_1')\n",
      " ((24202,), -13., 'umbita_1') ((24203,), -13., 'umbita_1')\n",
      " ((24204,), -13., 'umbita_1') ((24205,), -13., 'umbita_1')\n",
      " ((24206,), -13., 'umbita_1') ((24207,), -13., 'umbita_1')\n",
      " ((24208,), -13., 'umbita_1') ((24209,), -13., 'umbita_1')\n",
      " ((24210,), -13., 'umbita_1') ((24211,), -13., 'umbita_1')\n",
      " ((24212,), -13., 'umbita_1') ((24213,), -13., 'umbita_1')\n",
      " ((24214,), -13., 'umbita_1') ((24215,), -13., 'umbita_1')\n",
      " ((24238,), -16., 'umbita_2') ((24239,), -16., 'umbita_2')\n",
      " ((24240,), -16., 'umbita_2') ((24241,), -16., 'umbita_2')\n",
      " ((24242,), -16., 'umbita_2') ((24243,), -16., 'umbita_2')\n",
      " ((24244,), -16., 'umbita_2') ((24245,), -16., 'umbita_2')\n",
      " ((24246,), -16., 'umbita_2') ((24247,), -16., 'umbita_2')\n",
      " ((24248,), -16., 'umbita_2') ((24249,), -16., 'umbita_2')\n",
      " ((24266,), -12., 'chinavita_1') ((24267,), -12., 'chinavita_1')\n",
      " ((24268,), -12., 'chinavita_1') ((24269,), -12., 'chinavita_1')\n",
      " ((24270,), -12., 'chinavita_1') ((24271,), -12., 'chinavita_1')\n",
      " ((24272,), -12., 'chinavita_1') ((24273,), -12., 'chinavita_1')\n",
      " ((24274,), -12., 'chinavita_1') ((24302,), -14., 'tibana_1')\n",
      " ((24303,), -14., 'tibana_1') ((24333,), -17., 'tibana_2')\n",
      " ((24334,), -17., 'tibana_2') ((24335,), -17., 'tibana_2')\n",
      " ((24336,), -17., 'tibana_2') ((24337,), -17., 'tibana_2')\n",
      " ((24338,), -17., 'tibana_2') ((24339,), -17., 'tibana_2')\n",
      " ((24340,), -17., 'tibana_2') ((24341,), -17., 'tibana_2')\n",
      " ((24342,), -17., 'tibana_2') ((24343,), -17., 'tibana_2')\n",
      " ((24344,), -17., 'tibana_2') ((24345,), -17., 'tibana_2')\n",
      " ((24381,), -15., 'chinavita_2') ((24382,), -15., 'chinavita_2')\n",
      " ((24383,), -15., 'chinavita_2') ((24384,), -15., 'chinavita_2')\n",
      " ((24385,), -15., 'chinavita_2') ((24386,), -15., 'chinavita_2')\n",
      " ((24387,), -15., 'chinavita_2') ((24388,), -15., 'chinavita_2')\n",
      " ((24389,), -15., 'chinavita_2') ((24390,), -15., 'chinavita_2')\n",
      " ((24391,), -15., 'chinavita_2') ((24392,), -15., 'chinavita_2')\n",
      " ((24393,), -15., 'chinavita_2') ((24394,), -15., 'chinavita_2')\n",
      " ((24395,), -15., 'chinavita_2') ((24396,), -15., 'chinavita_2')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in wel.stress_period_data.array[0]:\n",
    "    node = data[0][0]\n",
    "    enum_name = data[2]\n",
    "    real_name = well_names[enum_name]\n",
    "    data[1] = wells_rate[real_name]\n",
    "    data[2] = real_name\n",
    "print(wel.stress_period_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMS package (Iterative Model Solution)\n",
    "This package contains the information related about numerical solver. This package is registers to simulation and it is assigned to each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package_name = mf6_webinar\n",
      "filename = mf6_webinar.ims\n",
      "package_type = ims\n",
      "model_or_simulation_package = simulation\n",
      "simulation_name = modflowsim\n",
      "\n",
      "Block options\n",
      "--------------------\n",
      "print_option\n",
      "{internal}\n",
      "(all)\n",
      "\n",
      "complexity\n",
      "{internal}\n",
      "(simple)\n",
      "\n",
      "\n",
      "Block nonlinear\n",
      "--------------------\n",
      "outer_hclose\n",
      "{internal}\n",
      "(0.01)\n",
      "\n",
      "outer_maximum\n",
      "{internal}\n",
      "(50)\n",
      "\n",
      "\n",
      "Block linear\n",
      "--------------------\n",
      "inner_maximum\n",
      "{internal}\n",
      "(30)\n",
      "\n",
      "inner_hclose\n",
      "{internal}\n",
      "(0.01)\n",
      "\n",
      "linear_acceleration\n",
      "{internal}\n",
      "(cg)\n",
      "\n",
      "relaxation_factor\n",
      "{internal}\n",
      "(0.97)\n",
      "\n",
      "preconditioner_levels\n",
      "{internal}\n",
      "(8)\n",
      "\n",
      "preconditioner_drop_tolerance\n",
      "{internal}\n",
      "(0.0001)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ims = sim.get_package('ims')\n",
    "print(ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of nonlinear interations: 50\n",
      "Maximum number of linear interations: 30\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of outer (nonlinear) iterations.\n",
    "print('Maximum number of nonlinear interations: {}'.format(ims.outer_maximum.get_data()))\n",
    "\n",
    "# Maximum number of inner (linear) iterations.\n",
    "print('Maximum number of linear interations: {}'.format(ims.inner_maximum.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of nonlinear interations: 500\n",
      "Maximum number of linear interations: 130\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of outer (nonlinear) iterations.\n",
    "ims.outer_maximum.set_data(500)\n",
    "print('Maximum number of nonlinear interations: {}'.format(ims.outer_maximum.get_data()))\n",
    "\n",
    "# Maximum number of inner (linear) iterations.\n",
    "ims.inner_maximum.set_data(130)\n",
    "print('Maximum number of linear interations: {}'.format(ims.inner_maximum.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity: simple\n"
     ]
    }
   ],
   "source": [
    "# Complexity: Defines if the model can trates as linear or nonlinear problem.\n",
    "print('Complexity: {}'.format(ims.complexity.get_data()))\n",
    "# simple: Model can be trated as linear problem (confined units, linear stress packages, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity: moderate\n"
     ]
    }
   ],
   "source": [
    "ims.complexity.set_data('moderate')\n",
    "print('Complexity: {}'.format(ims.complexity.get_data()))\n",
    "# moderate (several unconfined units, nonlinear stress packages, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton-Raphson solver\n",
    "This solver is recommended where traditional wet/drying numerical schemes does not get an acceptable solution due to convergence problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing simulation...\n",
      "  writing simulation name file...\n",
      "  writing simulation tdis package...\n",
      "  writing ims package mf6_webinar...\n",
      "  writing model mf6_webinar...\n",
      "    writing model name file...\n",
      "    writing package disu...\n",
      "    writing package riv_0...\n",
      "    writing package wel...\n",
      "    writing package npf...\n",
      "    writing package ic...\n",
      "    writing package oc...\n"
     ]
    }
   ],
   "source": [
    "# Newton-Rhapson solver using under relaxation \n",
    "model.name_file.newtonoptions = ('UNDER_RELAXATION')\n",
    "# This option allows to under-relax the head where water level falls below bottom of cell.\n",
    "\n",
    "# Change linear acceleration method \n",
    "ims.linear_acceleration.set_data('BICGSTAB')\n",
    "\n",
    "# write simulation\n",
    "sim.write_simulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FloPy is using the following  executable to run the model: C:/WRDAPP/mf6.0.4/mf6.0.4/bin/mf6.exe\n",
      "                                   MODFLOW 6\n",
      "                U.S. GEOLOGICAL SURVEY MODULAR HYDROLOGIC MODEL\n",
      "                            VERSION 6.0.4 03/13/2019\n",
      "\n",
      "   MODFLOW 6 compiled Mar 13 2019 12:37:09 with IFORT compiler (ver. 19.0.0)\n",
      "\n",
      "This software has been approved for release by the U.S. Geological \n",
      "Survey (USGS). Although the software has been subjected to rigorous \n",
      "review, the USGS reserves the right to update the software as needed \n",
      "pursuant to further analysis and review. No warranty, expressed or \n",
      "implied, is made by the USGS or the U.S. Government as to the \n",
      "functionality of the software and related material nor shall the \n",
      "fact of release constitute any such warranty. Furthermore, the \n",
      "software is released on condition that neither the USGS nor the U.S. \n",
      "Government shall be held liable for any damages resulting from its \n",
      "authorized or unauthorized use. Also refer to the USGS Water \n",
      "Resources Software User Rights Notice for complete use, copyright, \n",
      "and distribution information.\n",
      "\n",
      " Run start date and time (yyyy/mm/dd hh:mm:ss): 2019/07/23 18:41:39\n",
      "\n",
      " Writing simulation list file: mfsim.lst\n",
      " Using Simulation name file: mfsim.nam\n",
      " Solving:  Stress period:     1    Time step:     1\n",
      " Run end date and time (yyyy/mm/dd hh:mm:ss): 2019/07/23 18:59:02\n",
      " Elapsed run time: 17 Minutes, 23.334 Seconds\n",
      "\n",
      " Normal termination of simulation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, [])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run simulation\n",
    "sim.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results and save it on _vtk_ file\n",
    "The hydraulic heads are saved in the _.hds_ file. We can get this data using the output_keys command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mf6_webinar', 'CBC', 'FLOW-JA-FACE')\n",
      "('mf6_webinar', 'CBC', 'WEL')\n",
      "('mf6_webinar', 'CBC', 'RIV')\n",
      "('mf6_webinar', 'HDS', 'HEAD')\n"
     ]
    }
   ],
   "source": [
    "keys = sim.simulation_data.mfdata.output_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2547.28251739 2502.62520238 2483.49450224 ... 2247.43731625 2246.88894633\n",
      " 2246.5390188 ]\n"
     ]
    }
   ],
   "source": [
    "# get all head data\n",
    "head = sim.simulation_data.mfdata[file_name, 'HDS', 'HEAD']\n",
    "head = head[0]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that when Newton-Raphson method is used no cells will dry. Consequently, we have to mark all cell where head is below its bottom. For this, we create a numpy array called dry_cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "disu = model.get_package('disu') #First, we get DISU package from the model.\n",
    "bot = disu.bot.array # Second, we get cells bottom from DISU package.\n",
    "#mask = head < bot # Mask where val=True if head<bottom and val=False if not.\n",
    "\n",
    "dry_cells = np.array( head - bot, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_dry-cells_ is computed as _head - bot_. This array allows us to know which cells can be considerated as _dry_ (if _dry-cells<0_). Now we are ready to load the _vtk_ file and add the head and dry data to visalizate in Paraview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the vtk source file.\n",
    "reader = vtk.vtkUnstructuredGridReader()\n",
    "reader.SetFileName(sim_location + file_name + '.vtk')\n",
    "reader.Update() # Needed because of GetScalarRange\n",
    "ugrid = reader.GetOutput()\n",
    "reader.CloseVTKFile()\n",
    "\n",
    "# Convert head array to vtk_array and save it on untructured grid (ugrid)\n",
    "heads = numpy_support.numpy_to_vtk(head.ravel(), deep=True, array_type=vtk.VTK_FLOAT)\n",
    "heads.SetName('Heads')\n",
    "ugrid.GetCellData().AddArray(heads)\n",
    "\n",
    "# Convert dry_cells array to vtk_array and save it on untructured grid (ugrid)\n",
    "dry_cells = numpy_support.numpy_to_vtk(dry_cells.ravel(), deep=True, array_type=vtk.VTK_FLOAT)\n",
    "dry_cells.SetName('Dry')\n",
    "ugrid.GetCellData().AddArray(dry_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we update the _vtk_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writerVTK = vtk.vtkUnstructuredGridWriter()\n",
    "writerVTK.SetInputData(ugrid)\n",
    "writerVTK.SetFileName(sim_location + file_name + '.vtk')\n",
    "writerVTK.Update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading units nodes\n",
    "Geomodelr exports a _.json_ file that containts a dictionary where the keys are the geological units names and their values correponds to id nodes of the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file name:\n",
      "D:/CompartidaVB/Modflow/Webinar_2019/modflow6/mf6_webinar/mf6_webinar.json\n",
      "\n",
      "Geological units: dict_keys(['E2p', 'K2p', 'E1ss', 'K2E1g', 'K2lt', 'E1si', 'K1K2U', 'K2cp', 'E2E3co', 'E1c', 'K2d'])\n",
      "Number of nodes of E2p: 41030\n",
      "Number of nodes of E1ss: 16730\n",
      "Number of nodes of E2E3co: 2616\n",
      "\n",
      "First 20 nodes of E2E3co: [3170, 3171, 3192, 3996, 4015, 4034, 4053, 4072, 4073, 4092]\n"
     ]
    }
   ],
   "source": [
    "json_file = sim_location + file_name + '.json'\n",
    "print('JSON file name:\\n{}\\n'.format(json_file))\n",
    "\n",
    "with open(json_file) as json_file:  \n",
    "    unit_nodes = json.load(json_file)\n",
    "\n",
    "print('Geological units: {}'.format(unit_nodes.keys()))\n",
    "print('Number of nodes of E2p: {}'.format(len(unit_nodes[u'K2p'])))\n",
    "print('Number of nodes of E1ss: {}'.format(len(unit_nodes[u'E1ss'])))\n",
    "print('Number of nodes of E2E3co: {}'.format(len(unit_nodes[u'E2E3co'])))\n",
    "print('\\nFirst 20 nodes of E2E3co: {}'.format(unit_nodes[u'E2E3co'][:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
